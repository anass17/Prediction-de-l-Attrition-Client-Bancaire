{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7953f751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ML_Pipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"../data/processed/data-encoded.csv\", header=True, inferSchema=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a130902",
   "metadata": {},
   "source": [
    "### Gestion du déséquilibre de classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1e10b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "major = df.filter(df.Exited == 0)\n",
    "minor = df.filter(df.Exited == 1)\n",
    "\n",
    "ratio = minor.count() / major.count()\n",
    "major_sampled = major.sample(withReplacement=False, fraction=ratio)\n",
    "\n",
    "df_balanced = major_sampled.union(minor)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5c1dd42",
   "metadata": {},
   "source": [
    "### Sélection et assemblage des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df5b9ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer, OneHotEncoder, VectorAssembler\n",
    "\n",
    "# Step 1: index categorical columns\n",
    "indexers = [\n",
    "    StringIndexer(inputCol=\"Geography\", outputCol=\"Geography_index\"),\n",
    "    StringIndexer(inputCol=\"Gender\", outputCol=\"Gender_index\")\n",
    "]\n",
    "\n",
    "# Step 2: one-hot encode them\n",
    "encoders = [\n",
    "    OneHotEncoder(\n",
    "        inputCols=[\"Geography_index\", \"Gender_index\"],\n",
    "        outputCols=[\"Geography_vec\", \"Gender_vec\"]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Step 3: assemble numeric + encoded features\n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\"CreditScore\", \"Age\", \"Balance\", \"NumOfProducts\", \"Geography_vec\", \"Gender_vec\"],\n",
    "    outputCol=\"features\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bae869f",
   "metadata": {},
   "source": [
    "### Sélection et assemblage des features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "900dcf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler\n",
    "\n",
    "scaler = StandardScaler(\n",
    "    inputCol=\"features\",\n",
    "    outputCol=\"scaledFeatures\",\n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3883be5",
   "metadata": {},
   "source": [
    "### Séparation du dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6712ab54",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = df_balanced.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74027f27",
   "metadata": {},
   "source": [
    "### Choix du modèle MLlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "19469279",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"scaledFeatures\",\n",
    "    labelCol=\"Exited\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172557a7",
   "metadata": {},
   "source": [
    "### Construction du Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ecba5afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "pipeline = Pipeline(stages=[\n",
    "    assembler,\n",
    "    scaler,\n",
    "    lr\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07636e02",
   "metadata": {},
   "source": [
    "### Entraînement et évaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "329d3232",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "[FIELD_NOT_FOUND] No such struct field `Geography_vec` in `CreditScore`, `Geography`, `Gender`, `Age`, `Tenure`, `Balance`, `NumOfProducts`, `HasCrCard`, `IsActiveMember`, `EstimatedSalary`, `Exited`. SQLSTATE: 42704",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIllegalArgumentException\u001b[39m                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m model = \u001b[43mpipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m predictions = model.transform(test_df)\n\u001b[32m      4\u001b[39m predictions.select(\u001b[33m\"\u001b[39m\u001b[33mExited\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprediction\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mprobability\u001b[39m\u001b[33m\"\u001b[39m).show(\u001b[32m5\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anass\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\ml\\base.py:203\u001b[39m, in \u001b[36mEstimator.fit\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    201\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._fit(dataset)\n\u001b[32m    202\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m    206\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mParams must be either a param map or a list/tuple of param maps, \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    207\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mbut got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params)\n\u001b[32m    208\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anass\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\ml\\pipeline.py:136\u001b[39m, in \u001b[36mPipeline._fit\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(stage, Transformer):\n\u001b[32m    135\u001b[39m     transformers.append(stage)\n\u001b[32m--> \u001b[39m\u001b[32m136\u001b[39m     dataset = \u001b[43mstage\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    137\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# must be an Estimator\u001b[39;00m\n\u001b[32m    138\u001b[39m     model = stage.fit(dataset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anass\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\ml\\base.py:260\u001b[39m, in \u001b[36mTransformer.transform\u001b[39m\u001b[34m(self, dataset, params)\u001b[39m\n\u001b[32m    258\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy(params)._transform(dataset)\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    262\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mParams must be a param map but got \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m % \u001b[38;5;28mtype\u001b[39m(params))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anass\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\ml\\util.py:212\u001b[39m, in \u001b[36mtry_remote_transform_relation.<locals>.wrapped\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    210\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    211\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m212\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anass\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\ml\\wrapper.py:429\u001b[39m, in \u001b[36mJavaTransformer._transform\u001b[39m\u001b[34m(self, dataset)\u001b[39m\n\u001b[32m    426\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    428\u001b[39m \u001b[38;5;28mself\u001b[39m._transfer_params_to_java()\n\u001b[32m--> \u001b[39m\u001b[32m429\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m DataFrame(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_java_obj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m, dataset.sparkSession)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anass\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\py4j\\java_gateway.py:1362\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m   1361\u001b[39m answer = \u001b[38;5;28mself\u001b[39m.gateway_client.send_command(command)\n\u001b[32m-> \u001b[39m\u001b[32m1362\u001b[39m return_value = \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[32m   1366\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[33m\"\u001b[39m\u001b[33m_detach\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\anass\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pyspark\\errors\\exceptions\\captured.py:288\u001b[39m, in \u001b[36mcapture_sql_exception.<locals>.deco\u001b[39m\u001b[34m(*a, **kw)\u001b[39m\n\u001b[32m    284\u001b[39m converted = convert_exception(e.java_exception)\n\u001b[32m    285\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[32m    286\u001b[39m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[32m    287\u001b[39m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m288\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    290\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[31mIllegalArgumentException\u001b[39m: [FIELD_NOT_FOUND] No such struct field `Geography_vec` in `CreditScore`, `Geography`, `Gender`, `Age`, `Tenure`, `Balance`, `NumOfProducts`, `HasCrCard`, `IsActiveMember`, `EstimatedSalary`, `Exited`. SQLSTATE: 42704"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(train_df)\n",
    "predictions = model.transform(test_df)\n",
    "\n",
    "predictions.select(\"Exited\", \"prediction\", \"probability\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9966357d",
   "metadata": {},
   "source": [
    "### Évaluation du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7996e64e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "evaluator = BinaryClassificationEvaluator(\n",
    "    labelCol=\"Exited\",\n",
    "    rawPredictionCol=\"prediction\",\n",
    "    metricName=\"areaUnderROC\"\n",
    ")\n",
    "\n",
    "roc_auc = evaluator.evaluate(predictions)\n",
    "print(\"AUC =\", roc_auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8349f23",
   "metadata": {},
   "source": [
    "### Sauvegarde du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482b2619",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.write().overwrite().save(\"models/churn_model\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
